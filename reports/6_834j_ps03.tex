%##############################################################################
% Preamble
%##############################################################################

\documentclass{pset}
\name{Dustin Tran, Xiaomin Wang, Rodrigo Gomes}
\email{\{trandv,xiaominw, rgomes\}@mit.edu}

\course{6.834J/16.412J, S15}
\instructor{Professor Brian Williams}
\assignment{Problem Set \#3}
\duedate{April 17, 2015}

\begin{document}

%##############################################################################
% Begin Document
%##############################################################################

\section{Introduction}
Natural scenarios in real life occur where one must sequentially make decisions
under uncertainty. Quite often not only are transitions to certain states
unknown but the true state of the agent as well. This is similar to that of a
hidden Markov model (HMM), only in that one must make a sequence of actions
instead of a single action. The classic scenario is a robot navigating a
discrete environment using a GPS, and it takes action that lead to different
states with various probabilities, but because of the GPS there is also
inaccuracy in what its current state is.

One can formalize this, under Markovian assumptions, as a partially observable
Markov decision process (POMDP). In this project we provide a software
library for solving them with modular classes in order to allow for flexible
extensions.  More specifically, we encode a variety of basic tasks and solve
them using a combination of value iteration and a variant of Thompson sampling 
\cite{strens2000bayesian}, which is a Bayesian approach
following a Dirichlet-multinomial posterior over each state-action pair. 

\section{Technical Background}

\subsection{POMDP}
POMDP is a generalization of a Markov decision process (MDP). In an MDP, for each possible state of
the process, a decision has to be made regarding which action should be executed in that state. The
chosen action affects both the transition probabilities and the costs (or rewards) incurred. The
goal is to choose an optimal action in every state to increase some predefined measure of 
performance. In a POMDP model, the agent cannot fully observe the system states. 
Instead, it must maintain a probability distribution over the set of possible states, 
based on a set of observations and observation probabilities, and the underlying MDP.  

More formally, a POMDP is represented by the following variables: 

\begin{itemize}
\item $S$ is a set of states
\item $A$ is a set of actions
\item $T$ is a set of transition probabilities between states. If the agent is currently in state $s
\in S$, and it takes action $a \in A$. The agent will transition to some new state $s'$ according to 
$T(s' \mid s,a)$
\item $R: S \times A \rightarrow \mathbb{R}$ is a reward function that assigns a numeric reward (or
cost if the value is negative) for each state and action. 
\item $\Omega$ is a set of observations
\item $O$ is a set of conditional observation probabilites. If the agent is now in state $s$, 
it receives an observation $o$ according to $O(o \mid s)$
\item $\gamma \in [0,1]$ is a discount factor that determines how much rewards should be discounted over time
\end{itemize}
 
We ran this code in a sample environment we call gridworld: the agent has negative
reward for every state/action, except when it reaches a goal state, where is has
a high positive reward.
Gridworld represents a maze where the agent can be in discrete locations. Certain
locations are impossible for the agent, representing ``walls''. Every action can
move the agent between two adjacent grid locations, or fail, and keep the agent in
the same location with some probability.

\subsection{Value Iteration}

\subsection{Thompson Sampling}

The posterior probabilities are updated using the transition counts.

Thompson sampling gives us two capabilities: using dynamic programming, we can
find the optimal action to take at a certain state, and it allows us to learn
the transition model, and reward model for a certain environment. We tested this
with Markov Decision Processes (MDPs), and extended it to be used with POMDPs.

\section{Implementation}
For use with POMDPs, we assume that the agent is given an observation model of the
environment it is acting in, in the form of a conditional probability distribution
$P(observation \mid state)$. Astrom has shown that a properly updated probability 
distribution over the state space S is sufficient to summarize all the observable 
history of a POMDP agent without loss of optimality \cite{astrom}. The agent, however, is unaware of its transition model, and
reward model, and must learn those. Using these models, the agent keeps, and
updates a distribution over the states it can be in, and chooses the action with
the highest expected reward.


\subsection{Agent Environment Paradigm}

\subsection{Organization of Code}
We follow the directory structure specified in the problem set, with two
exceptions:
\begin{itemize}
\item \texttt{documentation/} does not exist. Instead, documentation is written
in the \texttt{README.md} inside the current working directory. Any additional
documentation not purely necessary for the problem set submission is in the
Github wiki.
\item \texttt{source/} is named \texttt{bayesrl/} in order to follow Python
convention for installing modules.
\end{itemize}

\section{Analysis}

\subsection{Runtime}

\subsection{Memory}

\subsection{Limitations}

\section{Benchmark Results}
The implementation of Thompson Sampling for MDPs ran very successfully: as time
progressed, the agent was able to get to the goal much faster, and get a high
reward. It did not work, however, for POMDPs. As time progressed, the agent seemed
to take about the same amount of time to reach the goal on every execution, not
improving in performance. We hypothesize that the reason is due to a lack of
a good prior on the transition model: thompson sampling relies a lot on being
able to count the number of transitions between states, given an action. This
is very hard in a POMDP with a weak prior on its transition model, as it may have
a completely wrong idea of where it ends up at each step. The observation model
is supposed to improve its accuracy, but it was not sufficient in this case.

To have an idea of how differently successful the same approach was on MDPs vs
POMDPs, we show you our results:


These graphs show the cumulative reward that the agent got as time progressed.
In the MDP case, the reward started low, and kept getting lower, as the agent
explored. It, however, started getting higher as the agent started acting in a
more deliberate manner, to maximize reward. The POMDP agent, however, seems to
be in a state where its reward just gets increasingly negative. It is unclear
whether it never leaves the exploration phase, or if it simply learns a completely
wrong transition model, and thus computes a very wrong policy.



\bibliography{6_834j_ps03}
\bibliographystyle{plain}

%##############################################################################
% End Document
%##############################################################################

\end{document}
